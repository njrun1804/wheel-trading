#!/usr/bin/env python3
"""
Comprehensive 30-second Bolt Performance Analysis
- Measures ops/sec, latency, and RAM usage
- Tests realistic workloads
- Validates against README/config limits
"""

import asyncio
import json
import sys
import time
from dataclasses import dataclass
from pathlib import Path
from typing import Any

import numpy as np
import psutil

# Add bolt to path
sys.path.insert(0, str(Path(__file__).parent))

# Optional GPU imports
try:
    import mlx.core as mx

    HAS_MLX = mx.metal.is_available()
except ImportError:
    HAS_MLX = False

try:
    import torch

    HAS_TORCH_MPS = torch.backends.mps.is_available()
except ImportError:
    HAS_TORCH_MPS = False


@dataclass
class BenchmarkResult:
    """Single benchmark result"""

    name: str
    ops_per_sec: float
    latency_ms: float
    peak_ram_mb: float
    success: bool
    error: str = None
    details: dict[str, Any] = None


class BoltPerformanceBenchmark:
    """Comprehensive 30-second Bolt performance test"""

    def __init__(self):
        self.results: list[BenchmarkResult] = []
        self.process = psutil.Process()
        self.initial_memory = self.process.memory_info().rss / 1024 / 1024

    def measure_memory(self) -> float:
        """Get current memory usage in MB"""
        return self.process.memory_info().rss / 1024 / 1024

    async def run_all_benchmarks(self) -> dict[str, Any]:
        """Run 30-second performance benchmark suite"""
        print("üöÄ Starting Bolt 30-Second Performance Analysis")
        print("=" * 60)

        start_time = time.time()

        benchmarks = [
            ("core_operations", self.benchmark_core_operations),
            ("memory_management", self.benchmark_memory_management),
            ("parallel_processing", self.benchmark_parallel_processing),
            ("gpu_acceleration", self.benchmark_gpu_acceleration),
            ("file_operations", self.benchmark_file_operations),
            ("realistic_workload", self.benchmark_realistic_workload),
        ]

        for name, benchmark_func in benchmarks:
            # Check if we're approaching 30 seconds
            elapsed = time.time() - start_time
            if elapsed > 25:  # Reserve 5 seconds for final processing
                print(f"‚è∞ Skipping {name} - approaching time limit")
                continue

            try:
                print(f"üìä Running {name}...")
                result = await benchmark_func()
                self.results.append(result)

                print(
                    f"   ‚úÖ {result.ops_per_sec:.1f} ops/sec, "
                    f"{result.latency_ms:.1f}ms latency, "
                    f"{result.peak_ram_mb:.1f}MB peak RAM"
                )

            except Exception as e:
                print(f"   ‚ùå Failed: {e}")
                self.results.append(
                    BenchmarkResult(
                        name=name,
                        ops_per_sec=0,
                        latency_ms=0,
                        peak_ram_mb=0,
                        success=False,
                        error=str(e),
                    )
                )

        total_duration = time.time() - start_time
        print(f"\n‚è±Ô∏è  Total benchmark time: {total_duration:.1f}s")

        return self.generate_performance_report()

    async def benchmark_core_operations(self) -> BenchmarkResult:
        """Test core computational operations"""
        start_memory = self.measure_memory()
        peak_memory = start_memory

        operations = 0
        start_time = time.perf_counter()

        # Matrix operations (core of many AI/ML tasks)
        for _ in range(10):
            size = 500
            a = np.random.rand(size, size).astype(np.float32)
            b = np.random.rand(size, size).astype(np.float32)
            np.dot(a, b)
            operations += 1

            current_memory = self.measure_memory()
            peak_memory = max(peak_memory, current_memory)

        # Array operations
        for _ in range(50):
            arr = np.random.randn(100000).astype(np.float32)
            np.sum(arr)
            np.std(arr)
            operations += 2

            current_memory = self.measure_memory()
            peak_memory = max(peak_memory, current_memory)

        duration = time.perf_counter() - start_time
        ops_per_sec = operations / duration
        latency_ms = (duration / operations) * 1000

        return BenchmarkResult(
            name="core_operations",
            ops_per_sec=ops_per_sec,
            latency_ms=latency_ms,
            peak_ram_mb=peak_memory - start_memory,
            success=True,
            details={"total_operations": operations, "duration": duration},
        )

    async def benchmark_memory_management(self) -> BenchmarkResult:
        """Test memory allocation and management"""
        start_memory = self.measure_memory()
        peak_memory = start_memory

        operations = 0
        start_time = time.perf_counter()

        # Test various allocation patterns
        arrays = []

        # Large allocations
        for _ in range(5):
            arr = np.random.rand(1000000).astype(np.float32)  # ~4MB each
            arrays.append(arr)
            operations += 1

            current_memory = self.measure_memory()
            peak_memory = max(peak_memory, current_memory)

        # Many small allocations
        for _ in range(100):
            arr = np.random.rand(1000).astype(np.float32)  # ~4KB each
            arrays.append(arr)
            operations += 1

            current_memory = self.measure_memory()
            peak_memory = max(peak_memory, current_memory)

        # Cleanup
        del arrays

        duration = time.perf_counter() - start_time
        ops_per_sec = operations / duration
        latency_ms = (duration / operations) * 1000

        return BenchmarkResult(
            name="memory_management",
            ops_per_sec=ops_per_sec,
            latency_ms=latency_ms,
            peak_ram_mb=peak_memory - start_memory,
            success=True,
            details={"peak_memory_mb": peak_memory},
        )

    async def benchmark_parallel_processing(self) -> BenchmarkResult:
        """Test parallel processing capabilities"""
        start_memory = self.measure_memory()
        peak_memory = start_memory

        async def cpu_task(task_id: int):
            """Simulate CPU-intensive task"""
            arr = np.random.randn(50000).astype(np.float32)
            return np.sum(arr * arr)

        operations = 0
        start_time = time.perf_counter()

        # Run tasks in parallel
        num_tasks = 12  # M4 Pro has 12 cores
        tasks = [cpu_task(i) for i in range(num_tasks)]

        results = await asyncio.gather(*tasks)
        operations = len(results)

        current_memory = self.measure_memory()
        peak_memory = max(peak_memory, current_memory)

        duration = time.perf_counter() - start_time
        ops_per_sec = operations / duration
        latency_ms = (duration / operations) * 1000

        return BenchmarkResult(
            name="parallel_processing",
            ops_per_sec=ops_per_sec,
            latency_ms=latency_ms,
            peak_ram_mb=peak_memory - start_memory,
            success=True,
            details={"num_parallel_tasks": num_tasks},
        )

    async def benchmark_gpu_acceleration(self) -> BenchmarkResult:
        """Test GPU acceleration if available"""
        start_memory = self.measure_memory()
        peak_memory = start_memory

        if not (HAS_MLX or HAS_TORCH_MPS):
            return BenchmarkResult(
                name="gpu_acceleration",
                ops_per_sec=0,
                latency_ms=0,
                peak_ram_mb=0,
                success=False,
                error="No GPU acceleration available",
            )

        operations = 0
        start_time = time.perf_counter()

        if HAS_MLX:
            # MLX operations
            for _ in range(5):
                size = 1000
                a = mx.random.normal((size, size))
                b = mx.random.normal((size, size))
                c = mx.matmul(a, b)
                mx.eval(c)  # Force evaluation
                operations += 1

                current_memory = self.measure_memory()
                peak_memory = max(peak_memory, current_memory)

        elif HAS_TORCH_MPS:
            # PyTorch MPS operations
            device = torch.device("mps")

            for _ in range(5):
                size = 1000
                a = torch.randn(size, size, device=device)
                b = torch.randn(size, size, device=device)
                c = torch.matmul(a, b)
                torch.mps.synchronize()
                operations += 1

                current_memory = self.measure_memory()
                peak_memory = max(peak_memory, current_memory)

        duration = time.perf_counter() - start_time
        ops_per_sec = operations / duration
        latency_ms = (duration / operations) * 1000

        backend = "mlx" if HAS_MLX else "torch_mps"

        return BenchmarkResult(
            name="gpu_acceleration",
            ops_per_sec=ops_per_sec,
            latency_ms=latency_ms,
            peak_ram_mb=peak_memory - start_memory,
            success=True,
            details={"backend": backend, "operations": operations},
        )

    async def benchmark_file_operations(self) -> BenchmarkResult:
        """Test file I/O operations"""
        start_memory = self.measure_memory()
        peak_memory = start_memory

        operations = 0
        start_time = time.perf_counter()

        # Test reading existing files
        test_files = list(Path("bolt").glob("**/*.py"))[:20]  # First 20 Python files

        for file_path in test_files:
            try:
                with open(file_path, encoding="utf-8") as f:
                    content = f.read()
                    len(content)  # Simple processing
                operations += 1

                current_memory = self.measure_memory()
                peak_memory = max(peak_memory, current_memory)

            except Exception:
                continue  # Skip files that can't be read

        duration = time.perf_counter() - start_time
        ops_per_sec = operations / duration if operations > 0 else 0
        latency_ms = (duration / operations) * 1000 if operations > 0 else 0

        return BenchmarkResult(
            name="file_operations",
            ops_per_sec=ops_per_sec,
            latency_ms=latency_ms,
            peak_ram_mb=peak_memory - start_memory,
            success=operations > 0,
            details={"files_processed": operations},
        )

    async def benchmark_realistic_workload(self) -> BenchmarkResult:
        """Test realistic Bolt workload simulation"""
        start_memory = self.measure_memory()
        peak_memory = start_memory

        operations = 0
        start_time = time.perf_counter()

        # Simulate typical Bolt operations:
        # 1. Code analysis
        # 2. Pattern matching
        # 3. Memory-intensive processing
        # 4. Parallel task execution

        async def analyze_code_pattern():
            """Simulate code analysis task"""
            # Generate synthetic "code" data
            code_tokens = np.random.randint(0, 1000, size=10000)
            # Simulate pattern matching
            patterns = np.random.randint(0, 100, size=50)
            matches = np.isin(code_tokens, patterns)
            return np.sum(matches)

        async def memory_processing_task():
            """Simulate memory-intensive processing"""
            data = np.random.randn(100000).astype(np.float32)
            # Simulate feature extraction
            features = np.fft.fft(data)
            return np.abs(features).mean()

        # Run mixed workload
        for _ in range(3):
            # Parallel analysis tasks
            analysis_tasks = [analyze_code_pattern() for _ in range(4)]
            processing_tasks = [memory_processing_task() for _ in range(2)]

            # Execute in parallel
            all_tasks = analysis_tasks + processing_tasks
            results = await asyncio.gather(*all_tasks)

            operations += len(results)

            current_memory = self.measure_memory()
            peak_memory = max(peak_memory, current_memory)

        duration = time.perf_counter() - start_time
        ops_per_sec = operations / duration
        latency_ms = (duration / operations) * 1000

        return BenchmarkResult(
            name="realistic_workload",
            ops_per_sec=ops_per_sec,
            latency_ms=latency_ms,
            peak_ram_mb=peak_memory - start_memory,
            success=True,
            details={"mixed_operations": operations},
        )

    def generate_performance_report(self) -> dict[str, Any]:
        """Generate comprehensive performance report"""
        successful_results = [r for r in self.results if r.success]
        failed_results = [r for r in self.results if not r.success]

        # Calculate aggregate metrics
        total_ops_per_sec = sum(r.ops_per_sec for r in successful_results)
        avg_latency = (
            np.mean([r.latency_ms for r in successful_results])
            if successful_results
            else 0
        )
        peak_ram = (
            max([r.peak_ram_mb for r in successful_results])
            if successful_results
            else 0
        )

        # Performance ratings (0-100 scale)
        performance_ratings = {}

        # Operations per second rating
        if total_ops_per_sec > 1000:
            ops_rating = 100
        elif total_ops_per_sec > 500:
            ops_rating = 80
        elif total_ops_per_sec > 100:
            ops_rating = 60
        elif total_ops_per_sec > 50:
            ops_rating = 40
        else:
            ops_rating = 20

        # Latency rating (lower is better)
        if avg_latency < 1:
            latency_rating = 100
        elif avg_latency < 5:
            latency_rating = 80
        elif avg_latency < 10:
            latency_rating = 60
        elif avg_latency < 50:
            latency_rating = 40
        else:
            latency_rating = 20

        # Memory efficiency rating
        if peak_ram < 100:
            memory_rating = 100
        elif peak_ram < 500:
            memory_rating = 80
        elif peak_ram < 1000:
            memory_rating = 60
        elif peak_ram < 2000:
            memory_rating = 40
        else:
            memory_rating = 20

        performance_ratings = {
            "throughput": ops_rating,
            "latency": latency_rating,
            "memory_efficiency": memory_rating,
            "overall": (ops_rating + latency_rating + memory_rating) / 3,
        }

        # System information
        system_info = {
            "cpu_count": psutil.cpu_count(),
            "cpu_count_logical": psutil.cpu_count(logical=True),
            "total_memory_gb": psutil.virtual_memory().total / 1024 / 1024 / 1024,
            "available_memory_gb": psutil.virtual_memory().available
            / 1024
            / 1024
            / 1024,
            "mlx_available": HAS_MLX,
            "torch_mps_available": HAS_TORCH_MPS,
        }

        # README/Config validation
        config_validation = self.validate_against_limits(total_ops_per_sec, peak_ram)

        return {
            "summary": {
                "total_benchmarks": len(self.results),
                "successful": len(successful_results),
                "failed": len(failed_results),
                "success_rate": len(successful_results) / len(self.results) * 100
                if self.results
                else 0,
            },
            "performance_metrics": {
                "total_ops_per_sec": total_ops_per_sec,
                "average_latency_ms": avg_latency,
                "peak_ram_mb": peak_ram,
                "initial_memory_mb": self.initial_memory,
            },
            "performance_ratings": performance_ratings,
            "system_info": system_info,
            "config_validation": config_validation,
            "detailed_results": [
                {
                    "name": r.name,
                    "ops_per_sec": r.ops_per_sec,
                    "latency_ms": r.latency_ms,
                    "peak_ram_mb": r.peak_ram_mb,
                    "success": r.success,
                    "error": r.error,
                    "details": r.details,
                }
                for r in self.results
            ],
            "failures": [{"name": r.name, "error": r.error} for r in failed_results],
        }

    def validate_against_limits(
        self, ops_per_sec: float, peak_ram_mb: float
    ) -> dict[str, Any]:
        """Validate performance against README/config limits"""

        # Expected limits for M4 Pro system
        expected_limits = {
            "min_ops_per_sec": 100,  # Minimum acceptable throughput
            "max_latency_ms": 100,  # Maximum acceptable latency
            "max_ram_mb": 4096,  # Maximum RAM usage (4GB)
            "target_ops_per_sec": 500,  # Target throughput
            "target_latency_ms": 10,  # Target latency
        }

        validation_results = {
            "meets_minimum_performance": ops_per_sec
            >= expected_limits["min_ops_per_sec"],
            "within_memory_limits": peak_ram_mb <= expected_limits["max_ram_mb"],
            "meets_target_performance": ops_per_sec
            >= expected_limits["target_ops_per_sec"],
            "expected_limits": expected_limits,
            "recommendations": [],
        }

        # Generate recommendations
        if ops_per_sec < expected_limits["min_ops_per_sec"]:
            validation_results["recommendations"].append(
                "Performance below minimum threshold - check CPU/GPU utilization"
            )

        if peak_ram_mb > expected_limits["max_ram_mb"]:
            validation_results["recommendations"].append(
                f"Memory usage ({peak_ram_mb:.1f}MB) exceeds recommended limit ({expected_limits['max_ram_mb']}MB)"
            )

        if ops_per_sec >= expected_limits["target_ops_per_sec"]:
            validation_results["recommendations"].append(
                "Excellent performance - system is well optimized"
            )

        return validation_results


async def main():
    """Run the comprehensive Bolt performance analysis"""
    benchmark = BoltPerformanceBenchmark()

    try:
        results = await benchmark.run_all_benchmarks()

        # Print detailed results
        print("\n" + "=" * 60)
        print("üìà PERFORMANCE ANALYSIS RESULTS")
        print("=" * 60)

        print("\nüéØ SUMMARY:")
        summary = results["summary"]
        print(
            f"   Benchmarks: {summary['successful']}/{summary['total_benchmarks']} successful"
        )
        print(f"   Success Rate: {summary['success_rate']:.1f}%")

        print("\n‚ö° PERFORMANCE METRICS:")
        metrics = results["performance_metrics"]
        print(f"   Total Throughput: {metrics['total_ops_per_sec']:.1f} ops/sec")
        print(f"   Average Latency: {metrics['average_latency_ms']:.1f} ms")
        print(f"   Peak RAM Usage: {metrics['peak_ram_mb']:.1f} MB")

        print("\nüìä PERFORMANCE RATINGS (0-100):")
        ratings = results["performance_ratings"]
        print(f"   Throughput: {ratings['throughput']:.1f}/100")
        print(f"   Latency: {ratings['latency']:.1f}/100")
        print(f"   Memory Efficiency: {ratings['memory_efficiency']:.1f}/100")
        print(f"   Overall Score: {ratings['overall']:.1f}/100")

        print("\nüñ•Ô∏è  SYSTEM INFO:")
        system = results["system_info"]
        print(
            f"   CPU Cores: {system['cpu_count']} physical, {system['cpu_count_logical']} logical"
        )
        print(f"   Total RAM: {system['total_memory_gb']:.1f} GB")
        print(f"   Available RAM: {system['available_memory_gb']:.1f} GB")
        print(f"   MLX Available: {system['mlx_available']}")
        print(f"   PyTorch MPS Available: {system['torch_mps_available']}")

        print("\n‚úÖ CONFIG VALIDATION:")
        validation = results["config_validation"]
        print(
            f"   Meets Minimum Performance: {validation['meets_minimum_performance']}"
        )
        print(f"   Within Memory Limits: {validation['within_memory_limits']}")
        print(f"   Meets Target Performance: {validation['meets_target_performance']}")

        if validation["recommendations"]:
            print("\nüí° RECOMMENDATIONS:")
            for rec in validation["recommendations"]:
                print(f"   ‚Ä¢ {rec}")

        if results["failures"]:
            print("\n‚ùå FAILURES:")
            for failure in results["failures"]:
                print(f"   ‚Ä¢ {failure['name']}: {failure['error']}")

        print("\nüìã DETAILED RESULTS:")
        for result in results["detailed_results"]:
            if result["success"]:
                print(
                    f"   ‚úÖ {result['name']}: {result['ops_per_sec']:.1f} ops/sec, "
                    f"{result['latency_ms']:.1f}ms, {result['peak_ram_mb']:.1f}MB"
                )
            else:
                print(f"   ‚ùå {result['name']}: {result['error']}")

        # Save results to file
        output_file = "bolt_performance_results.json"
        with open(output_file, "w") as f:
            json.dump(results, f, indent=2)

        print(f"\nüíæ Results saved to: {output_file}")

        # Overall assessment
        overall_score = ratings["overall"]
        if overall_score >= 80:
            print("\nüèÜ ASSESSMENT: EXCELLENT - Bolt is performing optimally")
        elif overall_score >= 60:
            print("\nüëç ASSESSMENT: GOOD - Bolt performance is acceptable")
        elif overall_score >= 40:
            print("\n‚ö†Ô∏è  ASSESSMENT: FAIR - Consider optimization")
        else:
            print(
                "\nüîß ASSESSMENT: NEEDS IMPROVEMENT - Performance optimization required"
            )

    except Exception as e:
        print(f"\nüí• Benchmark failed: {e}")
        import traceback

        traceback.print_exc()
        return 1

    return 0


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)
