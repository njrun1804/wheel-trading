"""Comprehensive test runner for BOB unified system.\n\nThis script runs all test categories and generates detailed reports.\n\"\"\"\n\nimport asyncio\nimport sys\nimport time\nimport json\nimport argparse\nfrom pathlib import Path\nfrom datetime import datetime\nfrom typing import Dict, List, Any, Optional\nimport subprocess\nimport pytest\nfrom dataclasses import dataclass, asdict\n\n# Add project root to Python path\nproject_root = Path(__file__).parent.parent\nsys.path.insert(0, str(project_root))\n\n\n@dataclass\nclass TestResult:\n    \"\"\"Test result data structure.\"\"\"\n    category: str\n    test_file: str\n    status: str  # passed, failed, skipped, error\n    duration: float\n    error_message: Optional[str] = None\n    details: Optional[Dict] = None\n\n\n@dataclass\nclass TestSuite:\n    \"\"\"Test suite configuration.\"\"\"\n    name: str\n    description: str\n    test_files: List[str]\n    markers: List[str]\n    dependencies: List[str]\n    timeout: int = 300  # 5 minutes default\n    parallel: bool = True\n\n\nclass BOBTestRunner:\n    \"\"\"Comprehensive test runner for BOB system.\"\"\"\n    \n    def __init__(self, output_dir: Optional[Path] = None):\n        self.output_dir = output_dir or Path(\"test_results\")\n        self.output_dir.mkdir(exist_ok=True)\n        \n        self.test_suites = self._define_test_suites()\n        self.results: List[TestResult] = []\n        self.start_time = None\n        self.end_time = None\n        \n    def _define_test_suites(self) -> Dict[str, TestSuite]:\n        \"\"\"Define all test suites for BOB system.\"\"\"\n        return {\n            'unit': TestSuite(\n                name='Unit Tests',\n                description='Individual component unit tests',\n                test_files=[\n                    'tests/unit/test_bob_config.py',\n                    'tests/unit/test_bob_core.py'\n                ],\n                markers=['unit'],\n                dependencies=[],\n                timeout=120,\n                parallel=True\n            ),\n            \n            'integration': TestSuite(\n                name='Integration Tests',\n                description='Einstein-BOB-Bolt integration tests',\n                test_files=[\n                    'tests/integration/test_einstein_bob_bolt_integration.py'\n                ],\n                markers=['integration'],\n                dependencies=['unit'],\n                timeout=600,\n                parallel=False\n            ),\n            \n            'performance': TestSuite(\n                name='Performance Tests',\n                description='Performance benchmarks and regression tests',\n                test_files=[\n                    'tests/performance/test_bob_performance_benchmarks.py'\n                ],\n                markers=['performance', 'slow'],\n                dependencies=['unit', 'integration'],\n                timeout=900,\n                parallel=False\n            ),\n            \n            'cli': TestSuite(\n                name='CLI Tests',\n                description='Command-line interface tests',\n                test_files=[\n                    'tests/cli/test_bob_cli_commands.py'\n                ],\n                markers=['cli'],\n                dependencies=['unit'],\n                timeout=300,\n                parallel=True\n            ),\n            \n            'error_handling': TestSuite(\n                name='Error Handling Tests',\n                description='Error handling and recovery tests',\n                test_files=[\n                    'tests/error_handling/test_bob_error_recovery.py'\n                ],\n                markers=['error_handling'],\n                dependencies=['unit', 'integration'],\n                timeout=450,\n                parallel=False\n            ),\n            \n            'hardware': TestSuite(\n                name='Hardware Acceleration Tests',\n                description='Hardware acceleration validation',\n                test_files=[\n                    'tests/hardware/test_bob_hardware_acceleration.py'\n                ],\n                markers=['gpu', 'performance'],\n                dependencies=['unit'],\n                timeout=600,\n                parallel=False\n            ),\n            \n            'trading': TestSuite(\n                name='Trading Integration Tests',\n                description='Trading system integration tests',\n                test_files=[\n                    'tests/trading/test_bob_trading_integration.py'\n                ],\n                markers=['trading'],\n                dependencies=['unit', 'integration'],\n                timeout=450,\n                parallel=True\n            ),\n            \n            'regression': TestSuite(\n                name='Regression Tests',\n                description='System regression validation',\n                test_files=[\n                    'tests/regression/test_bob_regression.py'\n                ],\n                markers=['regression'],\n                dependencies=['unit', 'integration', 'performance'],\n                timeout=300,\n                parallel=False\n            )\n        }\n    \n    async def run_all_tests(self, \n                           categories: Optional[List[str]] = None,\n                           skip_slow: bool = False,\n                           parallel_suites: bool = True) -> Dict[str, Any]:\n        \"\"\"Run all test suites and generate comprehensive report.\"\"\"\n        self.start_time = time.time()\n        \n        print(\"🚀 Starting BOB Comprehensive Test Suite\")\n        print(f\"📊 Output directory: {self.output_dir}\")\n        print(f\"⏰ Started at: {datetime.now().isoformat()}\")\n        print()\n        \n        # Filter test suites based on categories\n        if categories:\n            suites_to_run = {k: v for k, v in self.test_suites.items() if k in categories}\n        else:\n            suites_to_run = self.test_suites\n        \n        # Skip slow tests if requested\n        if skip_slow:\n            suites_to_run = {\n                k: v for k, v in suites_to_run.items() \n                if 'slow' not in v.markers\n            }\n        \n        # Run test suites\n        if parallel_suites:\n            await self._run_suites_parallel(suites_to_run)\n        else:\n            await self._run_suites_sequential(suites_to_run)\n        \n        self.end_time = time.time()\n        \n        # Generate comprehensive report\n        report = self._generate_comprehensive_report()\n        \n        # Save report\n        await self._save_report(report)\n        \n        # Print summary\n        self._print_summary(report)\n        \n        return report\n    \n    async def _run_suites_sequential(self, suites: Dict[str, TestSuite]):\n        \"\"\"Run test suites sequentially with dependency checking.\"\"\"\n        completed = set()\n        \n        # Sort by dependencies\n        sorted_suites = self._sort_suites_by_dependencies(suites)\n        \n        for suite_name in sorted_suites:\n            suite = suites[suite_name]\n            \n            # Check dependencies\n            if not all(dep in completed for dep in suite.dependencies):\n                print(f\"⚠️  Skipping {suite_name}: dependencies not met\")\n                continue\n            \n            print(f\"🧪 Running {suite.name}...\")\n            result = await self._run_test_suite(suite_name, suite)\n            \n            if result['status'] == 'passed':\n                completed.add(suite_name)\n            \n            print(f\"✅ {suite.name}: {result['status']} ({result['duration']:.1f}s)\")\n            print()\n    \n    async def _run_suites_parallel(self, suites: Dict[str, TestSuite]):\n        \"\"\"Run test suites in parallel where possible.\"\"\"\n        # Group suites by dependency levels\n        levels = self._group_suites_by_dependency_level(suites)\n        \n        for level, suite_names in levels.items():\n            print(f\"📋 Running Level {level} test suites...\")\n            \n            # Run suites at this level in parallel\n            tasks = []\n            for suite_name in suite_names:\n                suite = suites[suite_name]\n                if suite.parallel:\n                    task = asyncio.create_task(self._run_test_suite(suite_name, suite))\n                    tasks.append((suite_name, task))\n                else:\n                    # Run non-parallel suites individually\n                    result = await self._run_test_suite(suite_name, suite)\n                    print(f\"✅ {suite.name}: {result['status']} ({result['duration']:.1f}s)\")\n            \n            # Wait for parallel tasks\n            if tasks:\n                for suite_name, task in tasks:\n                    result = await task\n                    suite = suites[suite_name]\n                    print(f\"✅ {suite.name}: {result['status']} ({result['duration']:.1f}s)\")\n            \n            print()\n    \n    async def _run_test_suite(self, suite_name: str, suite: TestSuite) -> Dict[str, Any]:\n        \"\"\"Run a single test suite.\"\"\"\n        start_time = time.time()\n        \n        try:\n            # Build pytest command\n            cmd = self._build_pytest_command(suite)\n            \n            # Run tests\n            process = await asyncio.create_subprocess_exec(\n                *cmd,\n                stdout=asyncio.subprocess.PIPE,\n                stderr=asyncio.subprocess.PIPE,\n                cwd=project_root\n            )\n            \n            stdout, stderr = await asyncio.wait_for(\n                process.communicate(),\n                timeout=suite.timeout\n            )\n            \n            duration = time.time() - start_time\n            \n            # Parse results\n            return_code = process.returncode\n            \n            if return_code == 0:\n                status = 'passed'\n            elif return_code == 5:  # No tests collected\n                status = 'skipped'\n            else:\n                status = 'failed'\n            \n            # Record results\n            for test_file in suite.test_files:\n                result = TestResult(\n                    category=suite_name,\n                    test_file=test_file,\n                    status=status,\n                    duration=duration,\n                    error_message=stderr.decode() if stderr and status == 'failed' else None\n                )\n                self.results.append(result)\n            \n            return {\n                'status': status,\n                'duration': duration,\n                'stdout': stdout.decode(),\n                'stderr': stderr.decode()\n            }\n            \n        except asyncio.TimeoutError:\n            duration = time.time() - start_time\n            \n            for test_file in suite.test_files:\n                result = TestResult(\n                    category=suite_name,\n                    test_file=test_file,\n                    status='timeout',\n                    duration=duration,\n                    error_message=f\"Test suite timed out after {suite.timeout}s\"\n                )\n                self.results.append(result)\n            \n            return {\n                'status': 'timeout',\n                'duration': duration,\n                'stdout': '',\n                'stderr': f\"Timeout after {suite.timeout}s\"\n            }\n            \n        except Exception as e:\n            duration = time.time() - start_time\n            \n            for test_file in suite.test_files:\n                result = TestResult(\n                    category=suite_name,\n                    test_file=test_file,\n                    status='error',\n                    duration=duration,\n                    error_message=str(e)\n                )\n                self.results.append(result)\n            \n            return {\n                'status': 'error',\n                'duration': duration,\n                'stdout': '',\n                'stderr': str(e)\n            }\n    \n    def _build_pytest_command(self, suite: TestSuite) -> List[str]:\n        \"\"\"Build pytest command for test suite.\"\"\"\n        cmd = [\n            sys.executable, '-m', 'pytest',\n            '--tb=short',\n            '--no-header',\n            '-v'\n        ]\n        \n        # Add markers\n        if suite.markers:\n            marker_expr = ' or '.join(suite.markers)\n            cmd.extend(['-m', marker_expr])\n        \n        # Add output directory\n        report_file = self.output_dir / f\"{suite.name.lower().replace(' ', '_')}_report.xml\"\n        cmd.extend(['--junitxml', str(report_file)])\n        \n        # Add test files\n        cmd.extend(suite.test_files)\n        \n        return cmd\n    \n    def _sort_suites_by_dependencies(self, suites: Dict[str, TestSuite]) -> List[str]:\n        \"\"\"Sort test suites by dependencies.\"\"\"\n        sorted_suites = []\n        remaining = set(suites.keys())\n        \n        while remaining:\n            # Find suites with satisfied dependencies\n            ready = [\n                name for name in remaining\n                if all(dep in sorted_suites or dep not in suites for dep in suites[name].dependencies)\n            ]\n            \n            if not ready:\n                # Circular dependency or missing dependency\n                ready = list(remaining)  # Add remaining anyway\n            \n            for name in ready:\n                sorted_suites.append(name)\n                remaining.remove(name)\n        \n        return sorted_suites\n    \n    def _group_suites_by_dependency_level(self, suites: Dict[str, TestSuite]) -> Dict[int, List[str]]:\n        \"\"\"Group test suites by dependency level.\"\"\"\n        levels = {}\n        suite_levels = {}\n        \n        # Calculate dependency levels\n        for suite_name, suite in suites.items():\n            level = 0\n            for dep in suite.dependencies:\n                if dep in suite_levels:\n                    level = max(level, suite_levels[dep] + 1)\n            \n            suite_levels[suite_name] = level\n            \n            if level not in levels:\n                levels[level] = []\n            levels[level].append(suite_name)\n        \n        return levels\n    \n    def _generate_comprehensive_report(self) -> Dict[str, Any]:\n        \"\"\"Generate comprehensive test report.\"\"\"\n        total_duration = self.end_time - self.start_time if self.end_time and self.start_time else 0\n        \n        # Aggregate results by category\n        by_category = {}\n        for result in self.results:\n            if result.category not in by_category:\n                by_category[result.category] = {\n                    'passed': 0,\n                    'failed': 0,\n                    'skipped': 0,\n                    'error': 0,\n                    'timeout': 0,\n                    'total_duration': 0,\n                    'tests': []\n                }\n            \n            by_category[result.category][result.status] += 1\n            by_category[result.category]['total_duration'] += result.duration\n            by_category[result.category]['tests'].append(asdict(result))\n        \n        # Calculate overall stats\n        total_tests = len(self.results)\n        passed_tests = sum(1 for r in self.results if r.status == 'passed')\n        failed_tests = sum(1 for r in self.results if r.status == 'failed')\n        skipped_tests = sum(1 for r in self.results if r.status == 'skipped')\n        error_tests = sum(1 for r in self.results if r.status == 'error')\n        timeout_tests = sum(1 for r in self.results if r.status == 'timeout')\n        \n        pass_rate = (passed_tests / total_tests * 100) if total_tests > 0 else 0\n        \n        return {\n            'summary': {\n                'total_tests': total_tests,\n                'passed': passed_tests,\n                'failed': failed_tests,\n                'skipped': skipped_tests,\n                'errors': error_tests,\n                'timeouts': timeout_tests,\n                'pass_rate': pass_rate,\n                'total_duration': total_duration,\n                'start_time': datetime.fromtimestamp(self.start_time).isoformat() if self.start_time else None,\n                'end_time': datetime.fromtimestamp(self.end_time).isoformat() if self.end_time else None\n            },\n            'by_category': by_category,\n            'failed_tests': [\n                asdict(r) for r in self.results \n                if r.status in ['failed', 'error', 'timeout']\n            ],\n            'performance_metrics': self._extract_performance_metrics(),\n            'system_info': self._get_system_info(),\n            'recommendations': self._generate_recommendations()\n        }\n    \n    def _extract_performance_metrics(self) -> Dict[str, Any]:\n        \"\"\"Extract performance metrics from test results.\"\"\"\n        performance_tests = [\n            r for r in self.results \n            if 'performance' in r.category or 'benchmark' in r.test_file\n        ]\n        \n        if not performance_tests:\n            return {}\n        \n        return {\n            'avg_test_duration': sum(r.duration for r in performance_tests) / len(performance_tests),\n            'max_test_duration': max(r.duration for r in performance_tests),\n            'min_test_duration': min(r.duration for r in performance_tests),\n            'performance_test_count': len(performance_tests)\n        }\n    \n    def _get_system_info(self) -> Dict[str, Any]:\n        \"\"\"Get system information for the report.\"\"\"\n        import platform\n        import psutil\n        \n        return {\n            'platform': platform.platform(),\n            'python_version': platform.python_version(),\n            'cpu_count': psutil.cpu_count(),\n            'memory_gb': round(psutil.virtual_memory().total / (1024**3), 1),\n            'architecture': platform.machine()\n        }\n    \n    def _generate_recommendations(self) -> List[str]:\n        \"\"\"Generate recommendations based on test results.\"\"\"\n        recommendations = []\n        \n        failed_tests = [r for r in self.results if r.status in ['failed', 'error']]\n        timeout_tests = [r for r in self.results if r.status == 'timeout']\n        \n        if failed_tests:\n            recommendations.append(\n                f\"🔥 {len(failed_tests)} tests failed - investigate error messages and fix issues\"\n            )\n        \n        if timeout_tests:\n            recommendations.append(\n                f\"⏱️ {len(timeout_tests)} tests timed out - consider increasing timeouts or optimizing performance\"\n            )\n        \n        # Performance recommendations\n        performance_tests = [r for r in self.results if 'performance' in r.category]\n        if performance_tests:\n            avg_duration = sum(r.duration for r in performance_tests) / len(performance_tests)\n            if avg_duration > 60:\n                recommendations.append(\n                    \"⚡ Performance tests are slow - consider optimization opportunities\"\n                )\n        \n        # Coverage recommendations\n        total_tests = len(self.results)\n        if total_tests < 50:\n            recommendations.append(\n                \"📈 Consider adding more comprehensive test coverage\"\n            )\n        \n        pass_rate = sum(1 for r in self.results if r.status == 'passed') / total_tests * 100 if total_tests > 0 else 0\n        if pass_rate < 90:\n            recommendations.append(\n                \"🎯 Test pass rate below 90% - focus on test stability and reliability\"\n            )\n        \n        return recommendations\n    \n    async def _save_report(self, report: Dict[str, Any]):\n        \"\"\"Save comprehensive report to files.\"\"\"\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        \n        # Save JSON report\n        json_file = self.output_dir / f\"bob_test_report_{timestamp}.json\"\n        with open(json_file, 'w') as f:\n            json.dump(report, f, indent=2)\n        \n        # Save summary report\n        summary_file = self.output_dir / f\"bob_test_summary_{timestamp}.txt\"\n        with open(summary_file, 'w') as f:\n            f.write(self._format_summary_report(report))\n        \n        print(f\"📋 Reports saved to:\")\n        print(f\"   JSON: {json_file}\")\n        print(f\"   Summary: {summary_file}\")\n    \n    def _format_summary_report(self, report: Dict[str, Any]) -> str:\n        \"\"\"Format summary report as text.\"\"\"\n        summary = report['summary']\n        \n        lines = []\n        lines.append(\"BOB Unified System - Comprehensive Test Report\")\n        lines.append(\"=\" * 50)\n        lines.append(\"\")\n        lines.append(f\"Test Execution Summary:\")\n        lines.append(f\"  Total Tests: {summary['total_tests']}\")\n        lines.append(f\"  Passed: {summary['passed']} ({summary['pass_rate']:.1f}%)\")\n        lines.append(f\"  Failed: {summary['failed']}\")\n        lines.append(f\"  Skipped: {summary['skipped']}\")\n        lines.append(f\"  Errors: {summary['errors']}\")\n        lines.append(f\"  Timeouts: {summary['timeouts']}\")\n        lines.append(f\"  Total Duration: {summary['total_duration']:.1f}s\")\n        lines.append(\"\")\n        \n        # Category breakdown\n        lines.append(\"Test Categories:\")\n        for category, data in report['by_category'].items():\n            total_cat = sum(data[status] for status in ['passed', 'failed', 'skipped', 'error', 'timeout'])\n            pass_rate_cat = (data['passed'] / total_cat * 100) if total_cat > 0 else 0\n            lines.append(f\"  {category}: {data['passed']}/{total_cat} passed ({pass_rate_cat:.1f}%)\")\n        \n        lines.append(\"\")\n        \n        # Failed tests\n        if report['failed_tests']:\n            lines.append(\"Failed Tests:\")\n            for test in report['failed_tests']:\n                lines.append(f\"  ❌ {test['test_file']} ({test['status']})\")\n                if test['error_message']:\n                    lines.append(f\"     Error: {test['error_message'][:100]}...\")\n            lines.append(\"\")\n        \n        # Recommendations\n        if report['recommendations']:\n            lines.append(\"Recommendations:\")\n            for rec in report['recommendations']:\n                lines.append(f\"  {rec}\")\n            lines.append(\"\")\n        \n        # System info\n        sys_info = report['system_info']\n        lines.append(\"System Information:\")\n        lines.append(f\"  Platform: {sys_info['platform']}\")\n        lines.append(f\"  Python: {sys_info['python_version']}\")\n        lines.append(f\"  CPU Cores: {sys_info['cpu_count']}\")\n        lines.append(f\"  Memory: {sys_info['memory_gb']}GB\")\n        lines.append(f\"  Architecture: {sys_info['architecture']}\")\n        \n        return \"\\n\".join(lines)\n    \n    def _print_summary(self, report: Dict[str, Any]):\n        \"\"\"Print test summary to console.\"\"\"\n        summary = report['summary']\n        \n        print(\"\\n\" + \"=\" * 60)\n        print(\"🎯 BOB Unified System Test Results Summary\")\n        print(\"=\" * 60)\n        print()\n        print(f\"📊 Total Tests: {summary['total_tests']}\")\n        print(f\"✅ Passed: {summary['passed']} ({summary['pass_rate']:.1f}%)\")\n        print(f\"❌ Failed: {summary['failed']}\")\n        print(f\"⏭️  Skipped: {summary['skipped']}\")\n        print(f\"💥 Errors: {summary['errors']}\")\n        print(f\"⏱️  Timeouts: {summary['timeouts']}\")\n        print(f\"⏰ Duration: {summary['total_duration']:.1f}s\")\n        print()\n        \n        # Show category results\n        print(\"📋 Results by Category:\")\n        for category, data in report['by_category'].items():\n            total = sum(data[status] for status in ['passed', 'failed', 'skipped', 'error', 'timeout'])\n            pass_rate = (data['passed'] / total * 100) if total > 0 else 0\n            status_icon = \"✅\" if pass_rate == 100 else \"⚠️\" if pass_rate >= 80 else \"❌\"\n            print(f\"  {status_icon} {category}: {data['passed']}/{total} ({pass_rate:.1f}%)\")\n        print()\n        \n        # Show recommendations\n        if report['recommendations']:\n            print(\"💡 Recommendations:\")\n            for rec in report['recommendations']:\n                print(f\"  {rec}\")\n            print()\n        \n        # Overall result\n        if summary['pass_rate'] == 100:\n            print(\"🎉 All tests passed! BOB system is ready for production.\")\n        elif summary['pass_rate'] >= 90:\n            print(\"🚀 Most tests passed. BOB system is nearly ready for production.\")\n        elif summary['pass_rate'] >= 80:\n            print(\"⚠️  Some tests failed. Review issues before production deployment.\")\n        else:\n            print(\"🔥 Many tests failed. Significant issues need to be resolved.\")\n        \n        print(\"=\" * 60)\n\n\ndef main():\n    \"\"\"Main entry point for test runner.\"\"\"\n    parser = argparse.ArgumentParser(description=\"BOB Unified System Test Runner\")\n    parser.add_argument(\n        '--categories', \n        nargs='*',\n        choices=['unit', 'integration', 'performance', 'cli', 'error_handling', 'hardware', 'trading', 'regression'],\n        help='Test categories to run (default: all)'\n    )\n    parser.add_argument(\n        '--skip-slow',\n        action='store_true',\n        help='Skip slow-running tests'\n    )\n    parser.add_argument(\n        '--sequential',\n        action='store_true', \n        help='Run test suites sequentially instead of in parallel'\n    )\n    parser.add_argument(\n        '--output-dir',\n        type=Path,\n        help='Output directory for test results'\n    )\n    \n    args = parser.parse_args()\n    \n    # Create test runner\n    runner = BOBTestRunner(output_dir=args.output_dir)\n    \n    # Run tests\n    try:\n        report = asyncio.run(runner.run_all_tests(\n            categories=args.categories,\n            skip_slow=args.skip_slow,\n            parallel_suites=not args.sequential\n        ))\n        \n        # Exit with appropriate code\n        if report['summary']['pass_rate'] == 100:\n            sys.exit(0)  # All tests passed\n        elif report['summary']['failed'] == 0 and report['summary']['errors'] == 0:\n            sys.exit(0)  # Only skipped tests\n        else:\n            sys.exit(1)  # Some tests failed\n            \n    except KeyboardInterrupt:\n        print(\"\\n⚠️ Test execution interrupted by user\")\n        sys.exit(130)\n    except Exception as e:\n        print(f\"\\n💥 Test runner error: {e}\")\n        sys.exit(2)\n\n\nif __name__ == '__main__':\n    main()\n"