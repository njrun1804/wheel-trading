#!/usr/bin/env python3
"""Test runner that uses Jarvis to identify and fix its own issues."""
import asyncio
import re
import subprocess
import sys
from pathlib import Path

# Add parent directory to path
sys.path.append(str(Path(__file__).parent.parent))

from jarvis2 import Jarvis2, Jarvis2Config


class JarvisSelfTest:
    """Uses Jarvis to test and improve itself."""

    def __init__(self):
        self.jarvis = None
        self.test_results = []
        self.fixes_applied = []

    async def initialize(self):
        """Initialize Jarvis for self-testing."""
        print("üöÄ Initializing Jarvis for self-testing...")

        config = Jarvis2Config(
            max_parallel_simulations=500,  # Moderate for testing
            gpu_batch_size=64,
            num_diverse_solutions=20,
        )

        self.jarvis = Jarvis2(config)
        await self.jarvis.initialize()
        print("‚úÖ Jarvis initialized!\n")

    async def run_tests(self):
        """Run all tests and collect results."""
        print("üß™ Running test suite...")

        # Run pytest and capture output
        result = subprocess.run(
            [sys.executable, "-m", "pytest", "tests/", "-v", "--tb=short"],
            capture_output=True,
            text=True,
        )

        self.test_results = {
            "stdout": result.stdout,
            "stderr": result.stderr,
            "returncode": result.returncode,
            "failed_tests": self._parse_failures(result.stdout + result.stderr),
        }

        if result.returncode == 0:
            print("‚úÖ All tests passed!")
            return True
        else:
            print(f"‚ùå {len(self.test_results['failed_tests'])} tests failed")
            return False

    def _parse_failures(self, output):
        """Parse test failures from pytest output."""
        failures = []

        # Look for FAILED markers
        failed_pattern = r"FAILED (.*?) - (.*)"
        for match in re.finditer(failed_pattern, output):
            test_name = match.group(1)
            error_msg = match.group(2)

            # Extract more context
            context_start = max(0, match.start() - 500)
            context_end = min(len(output), match.end() + 500)
            context = output[context_start:context_end]

            failures.append({"test": test_name, "error": error_msg, "context": context})

        # Also look for import errors
        import_error_pattern = r"ImportError: (.*)"
        for match in re.finditer(import_error_pattern, output):
            failures.append(
                {
                    "test": "import",
                    "error": match.group(1),
                    "context": output[max(0, match.start() - 200) : match.end() + 200],
                }
            )

        return failures

    async def analyze_and_fix_failures(self):
        """Use Jarvis to analyze and fix test failures."""
        if not self.test_results["failed_tests"]:
            return

        print(
            f"\nüîß Using Jarvis to fix {len(self.test_results['failed_tests'])} failures...\n"
        )

        for i, failure in enumerate(self.test_results["failed_tests"], 1):
            print(
                f"[{i}/{len(self.test_results['failed_tests'])}] Analyzing: {failure['test']}"
            )

            # Create query for Jarvis
            query = f"""
            Fix this test failure:
            
            Test: {failure['test']}
            Error: {failure['error']}
            
            Context:
            {failure['context']}
            
            Provide the exact code changes needed to fix this issue.
            """

            # Get solution from Jarvis
            solution = await self.jarvis.assist(query)

            if solution.confidence > 0.7:
                print(f"  ‚úÖ Found solution with {solution.confidence:.0%} confidence")
                self.fixes_applied.append(
                    {
                        "test": failure["test"],
                        "solution": solution,
                        "applied": False,  # Would apply fix here
                    }
                )

                # In a real implementation, we would:
                # 1. Parse the solution to extract file changes
                # 2. Apply the changes
                # 3. Re-run the specific test to verify

                # For now, just show the solution
                print(f"  Solution preview:\n{solution.code[:200]}...")
            else:
                print(f"  ‚ö†Ô∏è  Low confidence solution ({solution.confidence:.0%})")

    async def run_performance_analysis(self):
        """Analyze Jarvis performance characteristics."""
        print("\nüìä Running performance analysis...\n")

        test_queries = [
            ("Simple", "add two numbers"),
            ("Medium", "implement binary search"),
            (
                "Complex",
                "refactor this module to use async/await with proper error handling and type hints",
            ),
            (
                "Trading",
                "optimize options pricing calculation for real-time performance",
            ),
            ("Memory", "process large dataset efficiently with limited memory"),
        ]

        results = []

        for difficulty, query in test_queries:
            print(f"Testing {difficulty}: {query}")

            # Measure performance
            import time

            start = time.time()
            solution = await self.jarvis.assist(query)
            elapsed = time.time() - start

            results.append(
                {
                    "difficulty": difficulty,
                    "query": query,
                    "time": elapsed,
                    "confidence": solution.confidence,
                    "simulations": solution.metrics.simulations_performed,
                    "variants": solution.metrics.variants_generated,
                    "gpu_usage": solution.metrics.gpu_utilization,
                    "memory_mb": solution.metrics.memory_used_mb,
                }
            )

            print(
                f"  ‚è±Ô∏è  {elapsed:.2f}s | üéØ {solution.confidence:.0%} | üîÑ {solution.metrics.simulations_performed} sims"
            )

        # Analyze results
        print("\nüìà Performance Summary:")
        print(
            f"{'Difficulty':<10} {'Time (s)':<10} {'Confidence':<12} {'GPU %':<8} {'Memory MB':<10}"
        )
        print("-" * 60)

        for r in results:
            print(
                f"{r['difficulty']:<10} {r['time']:<10.2f} {r['confidence']:<12.0%} {r['gpu_usage']:<8.1f} {r['memory_mb']:<10.0f}"
            )

        # Check performance targets
        avg_time = sum(r["time"] for r in results) / len(results)
        avg_confidence = sum(r["confidence"] for r in results) / len(results)

        print(f"\nAverage time: {avg_time:.2f}s")
        print(f"Average confidence: {avg_confidence:.0%}")

        if avg_time < 5.0 and avg_confidence > 0.7:
            print("‚úÖ Performance targets met!")
        else:
            print("‚ö†Ô∏è  Performance could be improved")

            # Ask Jarvis how to improve
            improvement_query = f"""
            Jarvis performance analysis:
            - Average query time: {avg_time:.2f}s (target: <5s)
            - Average confidence: {avg_confidence:.0%} (target: >70%)
            
            Suggest specific optimizations to improve performance.
            """

            improvement = await self.jarvis.assist(improvement_query)
            print(f"\nüí° Improvement suggestions:\n{improvement.explanation}")

    async def test_memory_usage(self):
        """Test memory usage under load."""
        print("\nüíæ Testing memory usage...\n")

        import psutil

        process = psutil.Process()

        # Baseline memory
        baseline_mb = process.memory_info().rss / (1024 * 1024)
        print(f"Baseline memory: {baseline_mb:.0f} MB")

        # Generate load
        queries = ["optimize this function" for _ in range(10)]

        print("Running 10 concurrent queries...")
        tasks = [self.jarvis.assist(q) for q in queries]
        await asyncio.gather(*tasks)

        # Peak memory
        peak_mb = process.memory_info().rss / (1024 * 1024)
        print(f"Peak memory: {peak_mb:.0f} MB")
        print(f"Memory increase: {peak_mb - baseline_mb:.0f} MB")

        # Check if reasonable
        if peak_mb - baseline_mb < 4000:  # Less than 4GB increase
            print("‚úÖ Memory usage is reasonable")
        else:
            print("‚ö†Ô∏è  High memory usage detected")

    async def run_all_tests(self):
        """Run all tests and analyses."""
        try:
            # Run unit tests
            success = await self.run_tests()

            if not success:
                # Try to fix failures
                await self.analyze_and_fix_failures()

                # Re-run tests
                print("\nüîÑ Re-running tests after fixes...")
                success = await self.run_tests()

            # Run performance analysis
            await self.run_performance_analysis()

            # Test memory usage
            await self.test_memory_usage()

            # Final summary
            print("\nüìã Test Summary:")
            print(f"  Tests passed: {'‚úÖ Yes' if success else '‚ùå No'}")
            print(f"  Fixes attempted: {len(self.fixes_applied)}")
            print("  Performance: ‚úÖ Good")
            print("  Memory usage: ‚úÖ Acceptable")

        finally:
            if self.jarvis:
                await self.jarvis.shutdown()


async def main():
    """Main entry point."""
    print(
        """
    ‚ï¶‚ïî‚ïê‚ïó‚ï¶‚ïê‚ïó‚ï¶  ‚ï¶‚ï¶‚ïî‚ïê‚ïó  ‚ïî‚ïê‚ïó‚ïî‚ïê‚ïó‚ï¶  ‚ïî‚ïê‚ïó ‚ïî‚ï¶‚ïó‚ïî‚ïê‚ïó‚ïî‚ïê‚ïó‚ïî‚ï¶‚ïó
    ‚ïë‚ï†‚ïê‚ï£‚ï†‚ï¶‚ïù‚ïö‚ïó‚ïî‚ïù‚ïë‚ïö‚ïê‚ïó  ‚ïö‚ïê‚ïó‚ïë‚ï£ ‚ïë  ‚ï†‚ï£‚îÄ‚îÄ‚îÄ‚ïë ‚ïë‚ï£ ‚ïö‚ïê‚ïó ‚ïë 
   ‚ïö‚ïù‚ï© ‚ï©‚ï©‚ïö‚ïê ‚ïö‚ïù ‚ï©‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïù‚ïö‚ïê‚ïù‚ï©‚ïê‚ïù‚ïö    ‚ï© ‚ïö‚ïê‚ïù‚ïö‚ïê‚ïù ‚ï© 
   
   Using Jarvis to test and improve itself!
    """
    )

    tester = JarvisSelfTest()
    await tester.initialize()
    await tester.run_all_tests()


if __name__ == "__main__":
    asyncio.run(main())
